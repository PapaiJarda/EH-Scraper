# -*- coding: UTF-8 -*-
# ==Headers==
# @Name:               EH Scraper
# @Description:        EH Scraper
# @Version:            0.0.1
# @Author:             dodying
# @Date:               2018-02-14 10:10:01
# @Last Modified by:   dodying
# @Last Modified time: 2018-02-14 12:08:42
# @Namespace:          https://github.com/dodying/EH-Scraper
# @SupportURL:         https://github.com/dodying/EH-Scraper/issues
# @Import:
# ==/Headers==

#@Name  EH Scraper
#@Key   EH_Scraper
#@Image EH.png
#@Hook  Books

import clr

clr.AddReference("System")
from System.IO import StreamReader
from System.Text import UTF8Encoding
# from System.Net import WebRequest
# from System.Text import Encoding

clr.AddReference("Ionic.Zip.dll")
from Ionic.Zip import ZipFile

# clr.AddReference('System.Web.Extensions')
# from System.Web.Script.Serialization import JavaScriptSerializer

import re

def EH_Scraper(books):
  for book in books:
    # ((cYo.Projects.ComicRack.Engine.ComicBook)book).FileLocation
    with ZipFile.Read(book.FileLocation) as zipfile:
      for i in zipfile.Entries:
        if re.search('info.txt', i.FileName):
          infoFile = i.FileName
      with StreamReader(zipfile[infoFile].OpenReader(), UTF8Encoding) as stream:
        contents = stream.ReadToEnd()

    info = parseInfoContent(contents)

    for i in info:
      setattr(book, i, info[i])

    # result = re.search('g/(\d+)/(\w+)/', contents)
    # gid = result.group(1)
    # token = result.group(2)

def parseInfoContent(text):
  info = {}
  text = re.sub('(Page|Image) \d+: .*','', text)
  text = re.sub('(Downloaded at|Generated by).*', '', text)
  text = re.sub('([\r\n]){2,}', '\n', text)
  text = re.sub('[\r\n]+$', '', text)
  text = re.sub('[\r\n]+> ', '\n', text)
  a = re.compile('[\r\n]+').split(text)
  b = {}

  for i in a:
    t = i.split(': ')
    if len(t) > 1:
      b[t[0]] = t[1]

  info['Title'] = a[0]
  t = re.sub('\[.*?\]|\(.*?\)|\{.*?\}|【.*?】', '', a[0])
  if re.search('\d+', t):
    info['Number'] = re.search('\d+', t).group()
  else:
    info['Number'] = '1'

  for i in a:
    if re.search('^http', i):
      info['Web'] = i
      break

  if 'parody' in b:
    info['Series'] = b['parody']

  if 'Uploader Comment:' in a:
    if a.index('Tags:') >= 0 and a.index('Uploader Comment:') < a.index('Tags:'):
      info['Summary'] = '\n'.join(a[a.index('Uploader Comment:') : a.index('Tags:')])
    else:
      info['Summary'] = '\n'.join(a[a.index('Uploader Comment:') :])

  if 'character' in b:
    info['Characters'] = b['character']

  if 'artist' in b:
    info['Writer'] = b['artist']
  elif 'group' in b:
    info['Writer'] = b['group']

  if re.search('FREE HENTAI', b['Category']):
    info['Genre'] = re.search('FREE HENTAI (.*?) GALLERY', b['Category']).group(1)
  else:
    info['Genre'] = b['Category']

  if re.search('Chinese', b['Language']):
    info['LanguageISO'] = 'zh'
  elif re.search('English', b['Language']):
    info['LanguageISO'] = 'en'
  else:
    info['LanguageISO'] = 'jp'

  if 'Rating' in b and re.search('[\d\.]+', b['Rating']):
    info['CommunityRating'] = float(b['Rating'])

  info['Tags'] = []
  for i in ['language', 'parody', 'character', 'group', 'artist', 'male', 'female', 'misc']:
    if i in b:
      t = re.compile(',(\s+|)').split(b[i])
      info['Tags'] = info['Tags'] + dArr(i, t)
  if len(info['Tags']) > 0:
    info['Tags'] = ','.join(info['Tags'])
  else:
    del info['Tags']

  if 'Posted' in b:
    date = re.search('(\d{4})-(\d{2})-(\d{2})', b['Posted'])
    info['Year'] = int(date.group(1))
    info['Month'] = int(date.group(2))
    info['Day'] = int(date.group(3))
    #info['Published'] = b['Posted']

  if 'Uploader' in b:
    info['Publisher'] = b['Uploader']


  return info

def scrapeFromEH(gid, token):
  parm = '{"method":"gdata","gidlist":[[' + gid + ',"' + token + '"]],"namespace":1}'

  req = WebRequest.Create("https://e-hentai.org/api.php")
  req.Method = "POST"
  req.ContentType = "application/x-www-form-urlencoded"
  req.UserAgent = "Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.87 Mobile Safari/537.36"

  parmBytes = Encoding.ASCII.GetBytes(parm)
  req.ContentLength = parmBytes.Length
  reqStream = req.GetRequestStream()
  reqStream.Write(parmBytes, 0, parmBytes.Length)
  reqStream.Close()

  response = req.GetResponse()
  result = StreamReader(response.GetResponseStream()).ReadToEnd()
  data = dict(JavaScriptSerializer().DeserializeObject(result))
  data = dict(data['gmetadata'][0])
  return data

def dArr(str, arr):
  for i in arr:
    if re.search('^\s+$', i):
      del arr[arr.index(i)]

  for i in arr:
    arr[arr.index(i)] = str + ': ' + i
  return arr

